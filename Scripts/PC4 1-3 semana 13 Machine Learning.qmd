---
title: "PC4 1/3"
author: "Katherine -Daniela Delgado"
format: html
editor: visual
---

## Instalar y cargar los paquetes

```{r}
install.packages("factoextra") 
install.packages("cluster")
```

```{r}
library(factoextra) 
library(cluster) 
library(here) 
library(rio) 
library(tidyverse)
```

# Machine Learning

Importaremos nuestros datos, en este caso estoy trabajando con la data de cirrosis la cual tiene mas variable numericas.

El data de cirrosis contiene informacion de 418 personas, con 20 variables donde incluye los dias de seguimiento, el estado, medicamento, edad, sexo, ascitis, hepatomegalia, aracnoides, edema, bilirrubina, colesterol, albumina, cobre, fosfatasa_alcalina, SGOT, trigliceridos, paquetas, tiempo_protrombina, etapa.

```{r}
data_cirrosis <- import(here("data2", "cirrosis.csv"))
```

```{r}
head (data_cirrosis)
```

## Preparación de los datos númericos

Ahora eliminamos las variables categóricas que no serán utilizadas en el análisis. Posteriormente, se conserva únicamente las variables numéricas de interés y se establece la columna **ID** como identificador de las filas, generando el objeto depurado **data_cirrosis1**.

```{r}
data_cirrosis1 = data_cirrosis |> 
  select(-Estado, -Medicamento, -Sexo, -Ascitis, -Aracnoides, -Edema, -Etapa, -Hepatomegalia) |> 
  column_to_rownames("ID")
```

### Estandarizar variables

Aca empleamos la función `scale()` para estandarizar las variables numéricas del conjunto de datos *data_cirrosis1*. Esta transformación ajusta cada variable a una media de 0 y una desviación estándar de 1, permitiendo que todas tengan la misma escala y evitando que variables con valores grandes dominen el análisis.

```{r}
cirrosis_data_escalado = scale(data_cirrosis1)
```

```{r}
cirrosis_data_escalado = scale(data_cirrosis1)
```

Un vistazo a los datos antes del escalamiento

```{r}
head(data_cirrosis1)
```

y un vistazo después del escalamiento:

```{r}
head(cirrosis_data_escalado)
```

##  Cálculo de distancias

ara saber qué pacientes se parecen más entre sí, primero tengo que calcular la distancia entre ellos. La función `dist()` hace esto automáticamente: compara cada par de pacientes y mide qué tan diferentes son según sus valores. Así se obtiene una matriz de distancias que luego sirve para formar los grupos.

```{r}
dist_data_cirrosis <- dist(cirrosis_data_escalado, method = "euclidean")
```

## Visualizando las distancias euclidianas con un mapa de calor

```{r}
fviz_dist(dist_data_cirrosis)
```

Este gráfico muestra la matriz de distancias entre todos los pacientes después de estandarizar los datos. Cada celda representa qué tan diferentes son dos pacientes: los colores más claros indican que están más cerca (más parecidos), mientras que los colores más oscuros muestran que están más lejos (menos parecidos). La diagonal aparece en un color uniforme porque cada paciente comparado consigo mismo tiene distancia cero. En general, este tipo de gráfico me permite visualizar patrones de similitud, grupos posibles y zonas donde los pacientes se parecen más entre sí.

## 1. El método de agrupamiento: función de enlace (linkage)

Se realizo el agrupamiento jerárquico usando el método *Ward.D2*. Este procedimiento toma la matriz de distancias entre los pacientes y empieza a agruparlos según su similitud. El método Ward.D2 busca formar grupos lo más homogéneos posible, minimizando la variabilidad dentro de cada grupo.

```{r}
dist_link_data_cirrosis <- hclust(d = dist_data_cirrosis, method = "ward.D2")
```

### Dendrogramas para la visualización de patrones

Los dendrogramas es una representación gráfica del árbol jerárquico generado por la función `hclust()`.

```{r}
fviz_dend(dist_link_data_cirrosis, cex = 0.7)
```

El dendrograma muestra cómo se van formando los grupos de pacientes según su similitud. En la parte inferior están todos los pacientes de manera individual, y conforme suben las ramas se van uniendo en grupos cada vez más grandes. Las uniones que ocurren a menor altura indican pacientes más parecidos entre sí, mientras que las que se unen a mayor altura representan grupos más diferentes. En este gráfico se observa que los grandes grupos recién se forman a alturas relativamente altas, lo que sugiere una estructura de varios subgrupos dentro de los datos. Este dendrograma es la base para decidir cuántos clústeres finales se deben cortar.

### ¿Cúantos grupos se formaron en el dendrograma?

El dendrograma muestra tres divisiones principales antes de las fusiones de mayor altura, lo que indica la presencia de tres conglomerados claramente diferenciados. Cortar a 3 grupos permite una interpretación más estable y consistente de la estructura de los datos. k = 3 define el número de clusters.

```{r}
fviz_dend(dist_link_data_cirrosis, 
          k = 3,
          cex = 0.5,
          k_colors = c("#2E9FDF", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE, 
          rect = TRUE)
```

La división en tres clústeres permite distinguir claramente tres perfiles de pacientes: uno con valores normales/intermedios, otro con parámetros elevados, y un tercero con marcadores disminuidos.

## 2. Agrupamiento con el algoritmo K-Means

## Estimando el número óptimo de clusters

Estandarizamos variables, como en el ejercicio anterior y escalamos los datos:

```{r}
cirrosis_data_escalado = scale(data_cirrosis1)
```

Como en la data tengo datos con NA, entonces los identificaremos y eliminaremos las filas con estos datos que no me permitiran trabajar con el ejercicio.

```{r}
colSums(is.na(cirrosis_data_escalado))
```

```{r}
cirrosis_data_escalado <- na.omit(cirrosis_data_escalado)
```

Ahora graficamos la suma de cuadrados dentro de los gráficos

```{r}
fviz_nbclust(cirrosis_data_escalado, kmeans, nstart = 25, method = "wss") + 
  geom_vline(xintercept = 3, linetype = 2)
```

-   De **1 a 2 clusters**, la caída del WSS es muy pronunciada.

-   De **2 a 3 clusters**, todavía se observa una reducción importante.

-   A partir de **k = 3**, la curva comienza a **“suavizarse”** y la mejora deja de ser tan marcada.

-   Desde **k = 4 en adelante**, la disminución del WSS es cada vez más leve, lo cual indica que agregar más clusters aporta poco.

    La línea punteada marca justamente **k = 3**, donde ocurre ese cambio.

    Según el método del codo, **el número óptimo de clusters es 3**, ya que representa el punto donde la curva deja de decrecer abruptamente y se estabiliza.

    Por lo tanto, **k = 3 es una elección estadísticamente adecuada**

## Cálculo del agrupamiento k-means

Como el algoritmo k-means puede generar resultados diferentes dependiendo de las asignaciones iniciales de los centroides, se utiliza el argumento `nstart = 25`. Esto le indica a R que pruebe 25 configuraciones iniciales distintas y que elija la que presente la menor variabilidad dentro de los clústeres. Normalmente, `nstart` tiene un valor por defecto de 1, pero se recomienda trabajar con valores más altos, como 20, 25 o incluso 50, para obtener un agrupamiento más estable y reproducible. En este caso, se utilizó un valor elevado justamente para asegurar una elección adecuada del número óptimo de clústeres.

```{r}
set.seed(123)
km_res <- kmeans(cirrosis_data_escalado, 3, nstart = 25)
```

```{r}
km_res
```

El análisis k-means agrupó a los pacientes en tres clústeres con tamaños de 169, 70 y 37 sujetos. Las medias estandarizadas muestran que el **clúster 1** presenta valores cercanos al promedio, con albumina y plaquetas un poco más altas y niveles más bajos de bilirrubina y fosfatasa alcalina, por lo que representa a los pacientes con un perfil más estable. El **clúster 2** agrupa pacientes con edad más alta, albumina baja y tiempo de protrombina elevado, reflejando un estado intermedio. En cambio, el **clúster 3** concentra los valores más elevados de bilirrubina, colesterol, SGOT, fosfatasa alcalina y triglicéridos, lo que sugiere un grupo más comprometido clínicamente. La proporción de varianza explicada por los tres grupos fue del 25.7%, lo cual es habitual en datos clínicos con alta variabilidad.

**Vector de clúster:** El listado largo que aparece muestra **a qué clúster pertenece cada paciente**.\
Por ejemplo, el paciente 1 está en el grupo 2, el paciente 2 en el grupo 1, etc.

## Visualización de los clústeres k-means

```{r}
fviz_cluster(
  km_res,
  data = cirrosis_data_escalado,
  palette = c("#2E9FDF", "#E7B800", "#FC4E07"),
  ellipse.type = "euclid",
  repel = TRUE,
  ggtheme = theme_minimal()
)
```

**Interpretacion:** El gráfico muestra la distribución de los pacientes en un espacio bidimensional obtenido mediante reducción de dimensiones (por ejemplo, PCA), donde cada punto representa a un individuo y su ubicación resume la información de todas las variables utilizadas en el análisis. Los colores identifican los tres clústeres formados por el algoritmo k-means.

El **clúster 1 (azul)** es el más grande y está más concentrado hacia la derecha, lo que indica que este grupo es bastante homogéneo y comparte características similares. El **clúster 2 (amarillo)** aparece más disperso en la parte inferior, mostrando que dentro de este grupo hay más variabilidad entre los pacientes. Por otro lado, el **clúster 3 (rojo)** está bien separado a la izquierda, formando un grupo más pequeño pero claramente distinto de los otros dos, lo que significa que sus pacientes tienen un perfil clínico más diferente. En general, el gráfico muestra que los tres clústeres se separan bien entre sí, lo que confirma que la clasificación fue adecuada.
